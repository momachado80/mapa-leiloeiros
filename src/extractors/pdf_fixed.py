"""
Extrator Limpo de Leiloeiros - Regras RÃ­gidas Anti-RuÃ­do
Extrai apenas nomes vÃ¡lidos ignorando marca d'Ã¡gua e cabeÃ§alhos.
"""
import pdfplumber
import re
import json
from pathlib import Path
from typing import List, Dict
import pandas as pd

class PDFCleanExtractor:
    """Extrai leiloeiros com regras rÃ­gidas anti-ruÃ­do"""
    
    def __init__(self, pdf_path: str = "docs/Leiloeiros de SP.pdf"):
        self.pdf_path = Path(pdf_path)
        self.noise_patterns = [
            'Adriano Duarte',
            'duarte.adriano',
            'Licensed to',
            '28255301828',
            'Matricula',
            'Posse',
            'MATRÃCULA',
            'POSSE',
            'NOME',
            'NOME DO LEILOEIRO'
        ]
        
    def is_noise(self, text: str) -> bool:
        """Verifica se o texto Ã© ruÃ­do/marca d'Ã¡gua"""
        if not text:
            return True
        
        text_lower = text.lower()
        
        # Verifica padrÃµes de ruÃ­do
        for pattern in self.noise_patterns:
            if pattern.lower() in text_lower:
                return True
        
        # Verifica se Ã© muito curto
        if len(text.strip()) < 3:
            return True
        
        # Verifica se contÃ©m nÃºmeros (nomes nÃ£o devem ter nÃºmeros)
        if re.search(r'\d', text):
            return True
        
        # Verifica se Ã© cabeÃ§alho da tabela
        if text_lower in ['matricula', 'posse', 'nome', 'nome do leiloeiro']:
            return True
        
        return False
    
    def extract_names_from_page(self, page) -> List[str]:
        """
        Extrai nomes de uma pÃ¡gina com bounding box que ignora cabeÃ§alho/rodapÃ©
        """
        page_height = page.height
        page_width = page.width
        
        # Define bounding box que ignora 10% do topo (cabeÃ§alho) e 10% da base (rodapÃ©)
        top_margin = page_height * 0.10
        bottom_margin = page_height * 0.90
        
        bbox = (0, top_margin, page_width * 0.35, bottom_margin)  # Apenas primeira coluna
        
        # Extrai texto da Ã¡rea definida
        cropped_page = page.within_bbox(bbox)
        text = cropped_page.extract_text() or ""
        
        # Processa linhas
        names = []
        for line in text.split('\n'):
            line = line.strip()
            
            # Ignora ruÃ­do
            if self.is_noise(line):
                continue
            
            # Remove espaÃ§os extras
            line = re.sub(r'\s+', ' ', line)
            
            # Remove nÃºmeros e caracteres especiais
            clean_line = re.sub(r'[\d\-/\.]+', '', line)
            clean_line = clean_line.strip()
            
            # ValidaÃ§Ã£o final
            if (clean_line and 
                len(clean_line) > 3 and 
                not re.search(r'\d', clean_line) and
                not self.is_noise(clean_line)):
                
                # Capitaliza nome
                words = clean_line.split()
                capitalized_words = []
                for word in words:
                    if word and len(word) > 1:
                        if word.isupper():
                            capitalized_words.append(word.title())
                        elif word.islower():
                            capitalized_words.append(word.capitalize())
                        else:
                            capitalized_words.append(word)
                
                final_name = ' '.join(capitalized_words)
                if final_name and len(final_name) > 3:
                    names.append(final_name)
        
        return names
    
    def extract_all_pages(self, page_limit: int = 19) -> List[Dict]:
        """Extrai dados de todas as pÃ¡ginas"""
        print(f"ğŸ“„ Extraindo dados limpos de: {self.pdf_path.name}")
        
        if not self.pdf_path.exists():
            print(f"âŒ Arquivo nÃ£o encontrado: {self.pdf_path}")
            return []
        
        all_data = []
        
        try:
            with pdfplumber.open(self.pdf_path) as pdf:
                total_pages = len(pdf.pages)
                pages_to_process = min(page_limit, total_pages)
                
                print(f"ğŸ“– Processando {pages_to_process} de {total_pages} pÃ¡ginas...")
                
                for page_num in range(pages_to_process):
                    print(f"   ğŸ” PÃ¡gina {page_num + 1}/{pages_to_process}...")
                    page = pdf.pages[page_num]
                    
                    # Extrai nomes da pÃ¡gina
                    names = self.extract_names_from_page(page)
                    
                    # Adiciona cada nome como registro
                    for name in names:
                        record = {
                            'nome': name,
                            'email': "",  # Vazio por padrÃ£o
                            'site': None,  # Null como solicitado
                            'pagina': page_num + 1,
                            'fonte': 'pdf_clean_extractor'
                        }
                        all_data.append(record)
                    
                    print(f"   âœ… {len(names)} nomes vÃ¡lidos encontrados")
                
                print(f"\nâœ… Total extraÃ­do: {len(all_data)} registros")
                
        except Exception as e:
            print(f"âŒ Erro ao processar PDF: {str(e)}")
            import traceback
            traceback.print_exc()
        
        return all_data
    
    def deduplicate_data(self, data: List[Dict]) -> List[Dict]:
        """Remove duplicados baseado no nome"""
        print("\nğŸ” Removendo duplicados...")
        
        seen_names = set()
        unique_data = []
        
        for record in data:
            nome = record['nome']
            if nome not in seen_names:
                seen_names.add(nome)
                unique_data.append(record)
        
        print(f"âœ… ApÃ³s deduplicaÃ§Ã£o: {len(unique_data)} registros Ãºnicos")
        return unique_data
    
    def save_raw_data(self, data: List[Dict], output_path: str = "data/processed/leiloeiros_limpos.json"):
        """Salva dados brutos em JSON"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Dados limpos salvos em: {output_path}")
        print(f"ğŸ“Š EstatÃ­sticas brutas:")
        print(f"   - Total de registros: {len(data)}")
        
        # Primeiros 10 nomes para verificaÃ§Ã£o
        print(f"\nğŸ” Primeiros 10 nomes extraÃ­dos:")
        for i, record in enumerate(data[:10], 1):
            print(f"   {i}. {record['nome']}")
        
        return output_file
    
    def run_extraction(self):
        """Executa extraÃ§Ã£o completa"""
        print("=" * 70)
        print("ğŸ”§ EXTRATOR LIMPO - REGRAS RÃGIDAS ANTI-RUÃDO")
        print("=" * 70)
        
        # Extrai dados
        raw_data = self.extract_all_pages(page_limit=19)  # Processa todas as pÃ¡ginas
        
        if not raw_data:
            print("âŒ Nenhum dado extraÃ­do")
            return None
        
        # Remove duplicados
        unique_data = self.deduplicate_data(raw_data)
        
        # Salva dados brutos
        output_path = self.save_raw_data(unique_data)
        
        # AnÃ¡lise
        print("\nğŸ“ˆ ANÃLISE DOS DADOS LIMPOS:")
        print("-" * 50)
        
        df = pd.DataFrame(unique_data)
        
        # DistribuiÃ§Ã£o por pÃ¡gina
        print(f"\nğŸ“– DistribuiÃ§Ã£o por pÃ¡gina:")
        page_counts = df['pagina'].value_counts().sort_index()
        for pagina, count in page_counts.items():
            print(f"   PÃ¡gina {pagina}: {count} leiloeiros")
        
        # Nomes mais longos (para verificaÃ§Ã£o)
        print(f"\nğŸ” Nomes mais longos (verificaÃ§Ã£o de qualidade):")
        df['nome_length'] = df['nome'].str.len()
        top_long_names = df.nlargest(5, 'nome_length')[['nome', 'pagina']]
        for idx, row in top_long_names.iterrows():
            print(f"   â€¢ {row['nome']} (PÃ¡gina {row['pagina']})")
        
        print(f"\nğŸ¯ Meta: ~613 leiloeiros")
        print(f"ğŸ“Š Atual: {len(unique_data)} leiloeiros extraÃ­dos")
        
        print("\nâœ… ExtraÃ§Ã£o limpa concluÃ­da!")
        return output_path

def main():
    """FunÃ§Ã£o principal"""
    extractor = PDFCleanExtractor()
    output_path = extractor.run_extraction()
    
    if output_path:
        print(f"\nğŸ“ Arquivo gerado: {output_path}")
        print(f"ğŸš€ PrÃ³ximo passo: Enriquecer com emails e sites")
        print(f"   python src/processors/enrich_leiloeiros.py")
    else:
        print("\nâŒ Falha na extraÃ§Ã£o")

if __name__ == "__main__":
    main()
