"""
Scraper para a Junta Comercial do Estado de SÃ£o Paulo (JUCE-SP)
Extrai lista de leiloeiros com nome, matrÃ­cula e site.

Nota: Como a URL pÃºblica exata da lista de leiloeiros nÃ£o foi identificada,
este script demonstra a estrutura completa e funcional do scraper.
Quando a URL correta for encontrada, basta atualizar a constante LEILOEIROS_URL.
"""
import asyncio
import json
import logging
from typing import List, Dict
from pathlib import Path

from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

# ConfiguraÃ§Ã£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class JUCESPScraper:
    """Scraper para o site da JUCE-SP"""
    
    # URL de exemplo - atualizar quando a URL real for encontrada
    LEILOEIROS_URL = "https://www.jucesponline.sp.gov.br"
    
    def __init__(self, headless: bool = True, max_pages: int = 5):
        """
        Inicializa o scraper.
        
        Args:
            headless: Se True, executa o navegador em modo headless
            max_pages: NÃºmero mÃ¡ximo de pÃ¡ginas para percorrer na paginaÃ§Ã£o
        """
        self.headless = headless
        self.max_pages = max_pages
        self.crawler = None
        
    async def __aenter__(self):
        """Context manager entry"""
        self.crawler = AsyncWebCrawler(headless=self.headless)
        await self.crawler.start()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        if self.crawler:
            await self.crawler.close()
    
    async def scrape_single_page(self, page_num: int = 1) -> List[Dict]:
        """
        Extrai leiloeiros de uma Ãºnica pÃ¡gina.
        
        Args:
            page_num: NÃºmero da pÃ¡gina
            
        Returns:
            Lista de dicionÃ¡rios com dados dos leiloeiros
        """
        print(f"ğŸ“„ Processando pÃ¡gina {page_num}...")
        
        try:
            # Para demonstraÃ§Ã£o, usamos dados de exemplo
            # Em produÃ§Ã£o, substituir pela extraÃ§Ã£o real da pÃ¡gina
            
            # Exemplo de como seria a extraÃ§Ã£o real:
            # strategy = JsonCssExtractionStrategy(
            #     css_selector="table tbody tr",
            #     schema={
            #         "nome": "td:nth-child(1)",
            #         "matricula": "td:nth-child(2)",
            #         "site": "td:nth-child(3) a@href"
            #     },
            #     multiple=True
            # )
            # 
            # result = await self.crawler.arun(
            #     url=f"{self.LEILOEIROS_URL}?page={page_num}",
            #     extraction_strategy=strategy,
            #     wait_for="table",
            #     timeout=30000
            # )
            
            # Simula delay de requisiÃ§Ã£o
            await asyncio.sleep(0.5)
            
            # Dados de exemplo para demonstraÃ§Ã£o
            if page_num == 1:
                return [
                    {"nome": "JOÃƒO DA SILVA LEILÃ•ES LTDA", "matricula": "12345/SP", "site": "https://www.joaodasilvaleiloes.com.br"},
                    {"nome": "MARIA OLIVEIRA AUCTION ME", "matricula": "67890/SP", "site": "https://www.mariaoliveiraleiloes.com.br"},
                    {"nome": "CARLOS SOUZA LEILOEIRO OFICIAL", "matricula": "54321/SP", "site": "https://www.carlossouzaleiloeiro.com.br"},
                ]
            elif page_num == 2:
                return [
                    {"nome": "ANA COSTA LEILÃ•ES E AVALIAÃ‡Ã•ES", "matricula": "98765/SP", "site": "https://www.anacostaleiloes.com.br"},
                    {"nome": "PEDRO ALVES LEILOEIRO REGISTRADO", "matricula": "13579/SP", "site": "https://www.pedroalvesleiloeiro.com.br"},
                    {"nome": "FERNANDA LIMA LEILÃ•ES SPE", "matricula": "24680/SP", "site": "https://www.fernandalemaleiloes.com.br"},
                ]
            else:
                return []
                
        except Exception as e:
            print(f"âœ— Erro na pÃ¡gina {page_num}: {str(e)}")
            return []
    
    async def scrape_all_pages(self) -> List[Dict]:
        """
        Percorre todas as pÃ¡ginas de leiloeiros.
        
        Returns:
            Lista completa de leiloeiros de todas as pÃ¡ginas
        """
        print("ğŸ” Iniciando extraÃ§Ã£o com paginaÃ§Ã£o...")
        
        all_leiloeiros = []
        
        for page_num in range(1, self.max_pages + 1):
            page_leiloeiros = await self.scrape_single_page(page_num)
            
            if not page_leiloeiros:
                print(f"â¹ï¸ Nenhum leiloeiro na pÃ¡gina {page_num}. Parando paginaÃ§Ã£o.")
                break
            
            all_leiloeiros.extend(page_leiloeiros)
            print(f"âœ… PÃ¡gina {page_num}: {len(page_leiloeiros)} leiloeiros")
            
            # Simula delay entre pÃ¡ginas (evitar rate limiting)
            if page_num < self.max_pages and page_leiloeiros:
                await asyncio.sleep(1)
        
        return all_leiloeiros
    
    async def test_real_connection(self) -> bool:
        """
        Testa a conexÃ£o com o site real.
        
        Returns:
            True se a conexÃ£o for bem-sucedida
        """
        print("ğŸŒ Testando conexÃ£o com o site da JUCE-SP...")
        
        try:
            result = await self.crawler.arun(
                url=self.LEILOEIROS_URL,
                timeout=10000
            )
            
            if result.success:
                print(f"âœ… ConexÃ£o bem-sucedida! HTML recebido: {len(result.html)} caracteres")
                
                # Verifica se parece ser uma pÃ¡gina de leiloeiros
                html_lower = result.html.lower()
                if any(term in html_lower for term in ['leiloeiro', 'jucesp', 'junta']):
                    print("âœ… Site identificado como JUCE-SP")
                return True
            else:
                print(f"âœ— Falha na conexÃ£o: {result.error_message}")
                return False
                
        except Exception as e:
            print(f"âœ— Erro de conexÃ£o: {str(e)}")
            return False
    
    def print_leiloeiros(self, leiloeiros: List[Dict]):
        """
        Imprime os leiloeiros no terminal para validaÃ§Ã£o.
        
        Args:
            leiloeiros: Lista de dicionÃ¡rios com dados dos leiloeiros
        """
        if not leiloeiros:
            print("ğŸ“­ Nenhum leiloeiro para exibir.")
            return
        
        print("\n" + "="*80)
        print("ğŸ“‹ LEILOEIROS EXTRAÃDOS")
        print("="*80)
        
        for i, leiloeiro in enumerate(leiloeiros, 1):
            print(f"\n{i}. {leiloeiro.get('nome', 'N/A')}")
            print(f"   ğŸ“ MatrÃ­cula: {leiloeiro.get('matricula', 'N/A')}")
            print(f"   ğŸŒ Site: {leiloeiro.get('site', 'N/A')}")
        
        print(f"\nğŸ“Š Total: {len(leiloeiros)} leiloeiros")
    
    def save_to_json(self, leiloeiros: List[Dict], filename: str = "data/raw/leiloeiros_sp.json"):
        """
        Salva os leiloeiros em arquivo JSON.
        
        Args:
            leiloeiros: Lista de dicionÃ¡rios com dados dos leiloeiros
            filename: Nome do arquivo JSON
        """
        if not leiloeiros:
            print("ğŸ“­ Nenhum dado para salvar.")
            return
        
        # Garante que o diretÃ³rio existe
        filepath = Path(filename)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        # Converte para JSON
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(leiloeiros, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Dados salvos em: {filepath}")
        
        # Mostra estatÃ­sticas
        print(f"ğŸ“ˆ EstatÃ­sticas:")
        print(f"   - Total de leiloeiros: {len(leiloeiros)}")
        print(f"   - Tamanho do arquivo: {filepath.stat().st_size} bytes")

async def main():
    """FunÃ§Ã£o principal para execuÃ§Ã£o do scraper"""
    print("=" * 60)
    print("ğŸ” SCRAPER JUCE-SP - Sistema de ExtraÃ§Ã£o de Leiloeiros")
    print("=" * 60)
    
    try:
        async with JUCESPScraper(headless=True, max_pages=3) as scraper:
            print("ğŸš€ Inicializando scraper...")
            
            # Testa conexÃ£o com o site real
            connection_ok = await scraper.test_real_connection()
            
            if connection_ok:
                print("\nâœ… Ambiente configurado corretamente!")
                print("ğŸ“ Para extraÃ§Ã£o real, atualize a URL em LEILOEIROS_URL")
                print("   e implemente a lÃ³gica de extraÃ§Ã£o em scrape_single_page()")
            
            print("\n" + "-" * 60)
            print("ğŸ”„ Iniciando extraÃ§Ã£o de dados (modo demonstraÃ§Ã£o)...")
            
            # Extrai os leiloeiros (modo demonstraÃ§Ã£o)
            leiloeiros = await scraper.scrape_all_pages()
            
            # Imprime os resultados no terminal
            scraper.print_leiloeiros(leiloeiros)
            
            # Salva em JSON
            if leiloeiros:
                scraper.save_to_json(leiloeiros)
                print("\nâœ… Scraping concluÃ­do com sucesso!")
                
                # InstruÃ§Ãµes para uso real
                print("\n" + "=" * 60)
                print("ğŸš€ PRÃ“XIMOS PASSOS PARA USO REAL:")
                print("=" * 60)
                print("1. Identifique a URL exata da lista de leiloeiros da JUCE-SP")
                print("2. Atualize a constante LEILOEIROS_URL no cÃ³digo")
                print("3. Implemente a extraÃ§Ã£o real em scrape_single_page()")
                print("4. Ajuste os seletores CSS conforme a estrutura da pÃ¡gina")
                print("5. Teste com uma Ãºnica pÃ¡gina antes de executar a paginaÃ§Ã£o completa")
            else:
                print("\nâš  Scraping concluÃ­do, mas nenhum leiloeiro foi encontrado.")
                
    except Exception as e:
        print(f"\nâŒ Erro durante execuÃ§Ã£o: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # Executa o scraper
    asyncio.run(main())
